{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.53978687e-05 1.23394576e-04 3.35350130e-04 9.11051194e-04\n",
      " 2.47262316e-03 6.69285092e-03 1.79862100e-02 4.74258732e-02\n",
      " 1.19202922e-01 2.68941421e-01 5.00000000e-01 7.31058579e-01\n",
      " 8.80797078e-01 9.52574127e-01 9.82013790e-01 9.93307149e-01\n",
      " 9.97527377e-01 9.99088949e-01 9.99664650e-01 9.99876605e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np ##this means you can access numpy functions by writing np.functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x--A scalar or numpy array of any size\n",
    "    Return:\n",
    "    s--sigmoid(x)\n",
    "    \"\"\"\n",
    "    ###start code here ###\n",
    "    s=1.0/(1+1/np.exp(x))\n",
    "    return s\n",
    "\n",
    "\n",
    "##sigmoid gradient function\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient(also call the slope of derivative) of the sigmoid function with respect to its input vector \n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate gradient\n",
    "    Arguments:\n",
    "    x--A scalar or numpy array\n",
    "    \n",
    "    Return :\n",
    "    ds---the computed gradient\n",
    "    \n",
    "    \"\"\"\n",
    "    ### store code here ###\n",
    "    s=1.0/ (1+1/np.exp(x))\n",
    "    ds=s*(1-s)\n",
    "    return ds\n",
    "\n",
    "\n",
    "x=range(-10,10)\n",
    "b=sigmoid(x)\n",
    "c=sigmoid_derivative(x)\n",
    "print(b)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,b)\n",
    "plt.plot(x,c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.67826139 0.29380381]\n",
      "  [0.90714982 0.52835647]\n",
      "  [0.4215251  0.45017551]]\n",
      "\n",
      " [[0.92814219 0.96677647]\n",
      "  [0.85304703 0.52351845]\n",
      "  [0.19981397 0.27417313]]\n",
      "\n",
      " [[0.60659855 0.00533165]\n",
      "  [0.10820313 0.49978937]\n",
      "  [0.34144279 0.94630077]]]\n",
      "[[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "## graded function :image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image--a numpy array of shape(length ,height,depth)\n",
    "    returns:\n",
    "    v--a vector of shape(length*height*depth,1)\n",
    "    \n",
    "    \"\"\"\n",
    "    ### start code here ###\n",
    "    v=image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    ### end code here\n",
    "    \n",
    "    return v\n",
    "\n",
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "print(image)\n",
    "b=image2vector(image)\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 4]\n",
      " [1 6 4]]\n",
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n",
      "[[0.         0.4472136  0.70710678]\n",
      " [1.         0.89442719 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "## graded function:normalizerows\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    Implement a function that normalizes each row of matrix x (to have unit length)\n",
    "    Argument:\n",
    "    x--A numpy matrix of shape(n,m)\n",
    "    \n",
    "    Returns:\n",
    "    x--the normalized(by row) numpy matrix .You are allowed to modify x\n",
    "    \n",
    "    \"\"\"\n",
    "    ###star code here###\n",
    "    #Compute x_norm as norm 2 of x. use np.linalg.norm(...,ord=2,axis=...)\n",
    "    x_norm=np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    #divide x by its norm.\n",
    "    x=x/x_norm ##broadcasting \n",
    "    ###end code here\n",
    "    return x\n",
    "\n",
    "## graded function:normalizeColumns\n",
    "def normalizeColumn(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    Implement a function that normalizes each column of matrix x (to have unit length)\n",
    "    Argument:\n",
    "    x--A numpy matrix of shape(n,m)\n",
    "    \n",
    "    Returns:\n",
    "    x--the normalized(by row) numpy matrix .You are allowed to modify x\n",
    "    \n",
    "    \"\"\"\n",
    "    ###star code here###\n",
    "    #Compute x_norm as norm 2 of x. use np.linalg.norm(...,ord=2,axis=...)\n",
    "    x_norm=np.linalg.norm(x,axis=0,keepdims=True)\n",
    "    #divide x by its norm.\n",
    "    x=x/x_norm ##broadcasting \n",
    "    ###end code here\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "b=normalizeRows(x)\n",
    "c=normalizeColumn(x)\n",
    "print(x)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x)=[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "##graded function:softmax\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax function for each row of input x.\n",
    "    code can work for a row vector and also for matrices of shape(n,m)\n",
    "    Argument:\n",
    "    x--A numpy matrix of shape(n,m)\n",
    "    \n",
    "    Returns:\n",
    "    s--A numpy matrix equal to the softmax of x,of shape (n,m)\n",
    "    \"\"\"\n",
    "    ###start code here###\n",
    "    #apply exp() element wise to x. \n",
    "    x_exp=np.exp(x) \n",
    "    #create a vector x_sum that sums each row of x_exp\n",
    "    x_sum=np.sum(x_exp,axis=1,keepdims=True) #(n,1)\n",
    "    #compute softmax(x) by dividing x_exp by x_sum .it should automatically use numpy broadcasting\n",
    "    s=x_exp/x_sum #(n,m)\n",
    "    ###end code here###\n",
    "    return s\n",
    "\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x)=\"+str(softmax(x)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vectorzation \n",
    "##in deep learning ,you deal with very large datasets,a non-computationanlly-optimal function can become a\n",
    "##huge bottleneck in your algorithm and can result in a model that takes ages to run. to make sure that your \n",
    "##code is computationally efficient ,you will use vectorization. for example ,try to tell the difference between\n",
    "##teh following implementations of the dot/outer/elementwise product.\n",
    "import time\n",
    "x1 = np.random.normal(loc=0,scale=1,size=50000000)\n",
    "x2 = np.random.normal(loc=0,scale=1,size=50000000)\n",
    "\n",
    "### classic dot product of vectors implementation\n",
    "tic=time.process_time()\n",
    "dot=0\n",
    "for i in range(len(x1)):\n",
    "    dot+=x1[i]*x2[i]\n",
    "toc=time.process_time()\n",
    "print(\"dot=\"+str(dot)+'\\n-----Computation time='+str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "tic1=time.process_time()\n",
    "dot=np.dot(x1,x2)\n",
    "toc1=time.process_time()\n",
    "print(\"dot=\"+str(dot)+'\\n-----Computation time='+str(1000*(toc1-tic1))+\"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vectorzation \n",
    "##in deep learning ,you deal with very large datasets,a non-computationanlly-optimal function can become a\n",
    "##huge bottleneck in your algorithm and can result in a model that takes ages to run. to make sure that your \n",
    "##code is computationally efficient ,you will use vectorization. for example ,try to tell the difference between\n",
    "##teh following implementations of the dot/outer/elementwise product.\n",
    "import time\n",
    "x1 = np.random.normal(loc=0,scale=1,size=50000)\n",
    "x2 = np.random.normal(loc=0,scale=1,size=50000)\n",
    "\n",
    "### classic dot product of vectors implementation\n",
    "tic=time.process_time()\n",
    "dot=0\n",
    "for i in range(len(x1)):\n",
    "    dot+=x1[i]*x2[i]\n",
    "toc=time.process_time()\n",
    "print(\"dot=\"+str(dot)+'\\n-----Computation time='+str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "tic1=time.process_time()\n",
    "dot=np.dot(x1,x2)\n",
    "toc1=time.process_time()\n",
    "print(\"dot=\"+str(dot)+'\\n-----Computation time='+str(1000*(toc1-tic1))+\"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5f9b80387507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlrutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset'"
     ]
    }
   ],
   "source": [
    "##Logistic regression with neural network \n",
    "##Build the general architecture of  a learning algorithm,including:\n",
    "#Initializing parameters\n",
    "#Calculating the cost function and its gradient\n",
    "#Using an optimization algorithm that is gradient descent\n",
    "#Gather all three functions above into main model function ,in the right order.\n",
    "\n",
    "### First ,let's run the cell below to import all the packages that you will need during this assignment.\n",
    "# numpy package is the fundametal package for scientific computing with Python\n",
    "#h5py package is a famous library to interact with a dataset that is stored on an H5 file.\n",
    "#matplotlib is a famous library to plot graphs in Python\n",
    "#PIL and scipy are used here to test you model with you own picture at the end \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "### Loading the data(cat /not cat)\n",
    "train_set_x_orig,train_set_y,test_set_x_orig,test_set_y,classes=load_dataset()\n",
    "#Example of a picture\n",
    "index=25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print(\"y=\"+str(train_set_y[:,index])+\", it's a '\"+classes[np.squeeze(train_set_y[:,index])].decode(\"utf-8\")+\"picture.\")\n",
    "\n",
    "##Many software bugs in deep learning come from having matrix /vector dimensions that don't fit.If you can keep\n",
    "##your matrix/vector dimensions straight you will go a long way toward eliminating many bugs\n",
    "# find the number of training examples /number of test examples/width of a training image\n",
    "#remember that train_set_x_orig is a tensors  shape (m_train, num_px, num_px, 3) ,so access number of training example\n",
    "#by writing train_set_x_orig.shape[0]\n",
    "\n",
    "###start code here ###\n",
    "m_train=train_set_x_orig.shape[0] # number of training examples\n",
    "m_test=test_set_x_orig.shape[0]#number of test examples\n",
    "num_px=train_set_x_orig.shape[1]#width of training image\n",
    "###end code here###\n",
    "print(\"Number of training examples:m_train=\"+str(m_train))\n",
    "print(\"Number of training examples:m_test=\"+str(n_test))\n",
    "print('Height/Width of each image:num_px='+str(num_px))\n",
    "print('Each image is of size:('+str(num_px)+','+str(num_px)+',3')\n",
    "print('train_set_x shape:'+str(train_set_x_orig.shape))\n",
    "print(\"train_set_y shape:\"+str(train_set_y.shape))\n",
    "print(\"test_set_x shape:\"+str(test_set_x.shape))\n",
    "print(\"test_set_y shape:\"+str(test_set_y.shape))\n",
    "\n",
    "###Reshape images fo shape(num_px,num_px,3)in a numpy-array of shape (num_px*numpx*3,1)\n",
    "\n",
    "### START CODE HERE ###\n",
    "train_set_x_flatten=train_set_x_orig.reshape(m_train,-1).T\n",
    "test_set_x_flatten=test_set_x_orig.reshape(m_test,-1).T\n",
    "###END CORE HERE\n",
    "\n",
    "###to represent color images ,the red green and blue channels(RGB) must be specified for pixel,and so the pixel value\n",
    "#is actually a vector of three numbers ranging from 0 to 255\n",
    "#one common preprocessing step in machine learning is to center and standardize your dataset,meaning that you substract\n",
    "#the mean of the whole numpy array from each example,and then divide each each exmaple by the standard deviation of the \n",
    "#whole numpy array.But for picture datasets,it is simpler and more convenient and works almost as well to just divide every \n",
    "#row of the datasets by 255(the maximum value of a pixel channel)\n",
    "#Let's standardize our dataset\n",
    "train_set_x=train_set_x_flatten/255\n",
    "test_set_x=test_set_x_flatten/255\n",
    "\n",
    "####General Architercture of learning algorithm\n",
    "##building the parts of our algorthm\n",
    "##the main steps for building a neural network \n",
    "#  --Define the model structure(such as number of input features /hidden layer/the width of the model)\n",
    "#--initialize the model's parameters\n",
    "#--learn the parameters for the model by minimizing the cost function loop\n",
    "           #-calculate current loss(forward propagation)\n",
    "           #-calculate current gradient (backword propagation)\n",
    "           #-update parameters (gradient descent)\n",
    "#-use the learned parameters to make predictions(on the test set)\n",
    "#-analyse the results and conclude\n",
    "\n",
    "import numpy as np ##this means you can access numpy functions by writing np.functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x--A scalar or numpy array of any size\n",
    "    Return:\n",
    "    s--sigmoid(x)\n",
    "    \"\"\"\n",
    "    ###start code here ###\n",
    "    s=1.0/(1+1/np.exp(x))\n",
    "    return s\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED FUNCTION:initialize_with_zeros\n",
    "import numpy as np\n",
    "def ini_parameters(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector  of zeros of shape(dim,1) for w and initializes b to 0.\n",
    "    Argument:\n",
    "    dim--size of the w vector we want (or number of parameters in this case)\n",
    "    Returns:\n",
    "    w--initialized vector of shape(dim,1)\n",
    "    b--initialized scalar( corresponds to the bias)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### start code here ###\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0\n",
    "    ###end code here###\n",
    "    assert(w.shape==(dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[[0.]\n",
      " [0.]]\n",
      "b=0\n"
     ]
    }
   ],
   "source": [
    "dim=2\n",
    "w,b=ini_parameters(dim)\n",
    "print(\"w=\"+str(w))\n",
    "print(\"b=\"+str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Forward and backword propagation \n",
    "\n",
    "#GRADED FUNCITON:PROPAGATE\n",
    "import numpy as np\n",
    "def propagate(w,b,X,Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient \n",
    "    \n",
    "    Arguments:\n",
    "    w--weight,a numpy array of size(num_px*num_px*3,1)\n",
    "    b--bias,a scalar\n",
    "    X--data of size(num_px*num_px*3,number of examples)\n",
    "    Y--ture \"label\" vector (containing 0 if not-cat,1 if cat) of size(1,number of examples)\n",
    "    \n",
    "    Return:\n",
    "    cost--negative log-likelihood cost for logistic regression\n",
    "    dw--gradient of the loss with respect to w,thus same shape as w\n",
    "    db--gradient of the loss with respect to b,thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    --Write your code step by step for the propagation .np.log(), np.dot()\n",
    "    \n",
    "    \"\"\"\n",
    "    m=X.shape(1)\n",
    "    #FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ###START CODE HERE###\n",
    "    A=sigmoid(np.dot(w.T,X)+b)  #compute activation Y HAT\n",
    "    cost=-(1.0/m)*np.sum(Y.np.log(A)+(1-Y)*np.log(1-A)) #compute cost funtion\n",
    "    ###END CODE HERE\n",
    "    \n",
    "    #BACKWARD PROPAGETION(TO FIND GRADIENT)\n",
    "    ###STAT CODE HERE###\n",
    "    dw=(1.0/m)*np.dot(X,(A-Y).T)\n",
    "    db=(1.0/m)*np.sum(A-Y)\n",
    "    ###END CODE HERE###\n",
    "    \n",
    "    assert(dw.shape==w.shape)\n",
    "    assert(dw.dtype==float)\n",
    "    cost=np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "    \n",
    "    grads={\"dw\":dw,\n",
    "           \"db\":db}\n",
    "    return grads,cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'propagate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34eb353b36b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dw='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"db=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'propagate' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads,cost=propagate(w,b,X,Y)\n",
    "print('dw='+str(grads['dw']))\n",
    "print(\"db=\"+str(grads['db']))\n",
    "print(\"cost=\"+str(cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##optimization \n",
    "##you have initialized your parameters\n",
    "##compute cost function and its gradient\n",
    "##update the parameters using gradient descent.\n",
    "#GRADED FUNCTION:OPTIMIZE\n",
    "def optimize(w,b,X,Y,num_iterations,learning_rate,print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w--weights,a numpy array of size (num_px*num_px*3,1)\n",
    "    b--bias,a scalar\n",
    "    X--data of shape(num_px*num_px*3,number of examples)\n",
    "    Y--true \"label\" vector (containing 0 if not-cat,1 if cat),of shape (1,number of examples)\n",
    "    num_iterations--number of iterations of the optimization loop\n",
    "    learning_rate--learning rate of the gardient descent update rule\n",
    "    print_cost--True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params--dictionary containing the weights w and bias b\n",
    "    grads--dictionary containing the gardients of the weights and bias with respect to the cost function\n",
    "    costs--list of all the costs computed during the optimizaiton ,this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "         !) Calculate the cost and gradient for the current parameters.Use propagate().\n",
    "         2)Update the parameters using gradient descent rule for w and b.\n",
    "         \n",
    "    \"\"\"\n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        #Cost and gradient calculation\n",
    "        ###START CODE HERE###\n",
    "        grads,cost=propagate(w,b,X,Y)\n",
    "        ###END CODE HERE###\n",
    "        \n",
    "        #Retrieve derivatives from grads\n",
    "        dw=grads['dw']\n",
    "        db=grads['db']\n",
    "        \n",
    "        #update rule\n",
    "        w=w-learning_rate*dw\n",
    "        b=b-learning_rate*db\n",
    "        #end code here\n",
    "        \n",
    "        #Record the costs\n",
    "        if i%100==0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        #Print the cost every 100 training examples\n",
    "        if print_cost and i%100==0:\n",
    "            print(\"Cost after iteration %i:%f\"%(i,cost))\n",
    "            \n",
    "        params={ \"w\":w,\n",
    "                 \"b\":b}\n",
    "        grads={\"dw\":dw,\n",
    "               \"db\":db}\n",
    "        return params,grads,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-51a7b713752b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.09\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dw=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"db=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-55373babfb24>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#Cost and gradient calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m###START CODE HERE###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m###END CODE HERE###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d6668a2e13d0>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#FORWARD PROPAGATION (FROM X TO COST)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m###START CODE HERE###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "params,grads,costs=optimize(w,b,X,Y,num_iterations=100,learning_rate=0.09,print_cost=False)\n",
    "print(\"w=\"+str(params[\"w\"]))\n",
    "print(\"b=\"+str(params[\"b\"]))\n",
    "print(\"dw=\"+str(grads['dw']))\n",
    "print(\"db=\"+str(grads['db']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The privious function will output the learned w and b.We are able to use w and b to predict the  labels \n",
    "# for dataset X。Implement the predict() function .There is two steps to computing predictions\n",
    "#1. Calculate y hat=A(prob)\n",
    "#2.convert the entries of a  into 0 (if activation<=0.5) or 1(if activation>0.5) ,stores the predictions in a vector\n",
    "import numpy as np\n",
    "###GRADED FUNCTION:PREDICT\n",
    "def predict(w,b,X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters\n",
    "    \n",
    "    Arguments:\n",
    "    w--weights, a numpy array of size(num_px,num_px*3,1)\n",
    "    b--bias,a scalar\n",
    "    x--data of size(num_px*num_px*3,number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction--a numpy array (vecto) containing all predictions(0/1) for the examples in X\n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    Y_prediction=np.zeros((1,m))\n",
    "    w=w.reshape(X.shape[0],1)\n",
    "    #Compute vector \"A\" predicting the probabilities of cat being present in the picture \n",
    "    ###start code here###\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "    ###END CODE HERE###\n",
    "    for i in range(A.shape[1]):\n",
    "        #Convert probabilities A[0,i] to actual preditions p[0,i]\n",
    "        ###START CODE HERE###\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "        else:\n",
    "            Y_prediction[0,i]=0\n",
    "        ###END CODE HERE###\n",
    "    assert(Y_prediction.shape==(1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-1fe61cf11f1e>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1fe61cf11f1e>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    def model(x_train,y_train,x_test,y_test,num_iterations=2000,learning_rate=0，print_cost=False):\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "##MERGE ALL FUNCTIONS INTO A MODEL\n",
    "#You've implemented several functions that:\n",
    "#-Initialize(w,b)\n",
    "#-Optimize the loss iteratively to learn parameters(w,b):\n",
    "#-Computing the cost and its gradient \n",
    "#-Updating the parameters using gradient descent\n",
    "#-Use the learned(w,b) to predict the labels for geien set of examples\n",
    "#You will now see how the overall model is structured by putting together all building blocks(funtions implement\n",
    "# in the previous parts) together ,in the right order.\n",
    "###GRADED FUNCITON:MODEL\n",
    "import numpy as np\n",
    "def model(x_train,y_train,x_test,y_test,num_iterations=2000,learning_rate=0，print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    x_train--training set represented by numpy array of shape(num_px*num_px*3,m_train)\n",
    "    y_train--training set represented by numpy array of shape(1,m_train)\n",
    "    x_test--test set represented by numpy array of shape (num_px*num_px*3,m_test)\n",
    "    y_test--test set represented by numpy array of shape (1,m_test)\n",
    "    num_iterations--hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate--hyperarameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost--set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d--dictinary containing information about the model\n",
    "    \"\"\"\n",
    "    ###START CODE HERE###\n",
    "    #initialize parameters with zeros()\n",
    "    w,b=intialize_with_zero(x_train.shape[0])\n",
    "    \n",
    "    #Gradient descent\n",
    "    parameters,grads,costs=optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n",
    "    \n",
    "    #Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w=parameters['w']\n",
    "    b=parameters['b']\n",
    "    \n",
    "    #predict test/train set examples \n",
    "    y_prediction_test=predict(w，b,x_test)\n",
    "    y_prediction_train=predict(w,b,x_train)\n",
    "    \n",
    "    #end code here#\n",
    "    \n",
    "    \n",
    "    #Print train/test \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.basemap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db2649dcf0a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.basemap'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
